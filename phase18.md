 네이버 검색로봇이 웹페이지에 접근할 수 없습니다.
네이버 검색로봇이 웹페이지를 방문할 수 있도록 허용해 주세요.
웹 서버의 응답속도가 너무 느리거나 방화벽으로 인하여 네이버 검색로봇의 접근이 불가능할 수 있습니다. 웹 서버의 응답은 가급적 10초 이내로 응답할 수 있게 최적화 해주세요.
User Agent 및 IP 정보를 참고하여 네이버 검색로봇이 사이트에 접근할 수 있는지 점검해보세요.


웹마스터 가이드
/
/
검색로봇 확인 방법
검색로봇 확인 방법
웹페이지를 방문하는 프로그램이나 앱은 자기 자신만의 이름을 User-Agent에 명시합니다. 통상의 웹 브라우저 소프트웨어뿐만이 아니라 검색로봇도 자신만의 User-Agent 이름을 가지고 있습니다.

많은 웹사이트들은 일부 악의적인 접근을 막기 위하여 방화벽을 운영하고 있습니다. 만약 내 사이트가 검색에 노출되지 않거나 정보가 변경되지 않는다면, 이러한 목적을 위해 활동하는 로봇이 User-Agent 이름이나 IP 주소 등 방화벽의 제한 설정 때문에 접근이 차단되지는 않았는지 점검해야합니다.

네이버가 운용하는 로봇인지를 구분하기 위해서는 User-Agent를 활용하여 구분하는 방법과 역 DNS 조회를 통한 IP로 확인하는 방법이 있습니다.

User-Agent 이름으로 확인
일반적인 네이버 검색 서비스를 위해 웹 문서를 수집하는 검색로봇은 Yeti라고 불리우며,
아래와 같은 User-Agent 이름을 사용합니다. 세부 버전 등은 사전 예고 없이 변경될 수 있습니다.

Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko; compatible; Yeti/1.1; +https://naver.me/spd) Chrome/W.X.Y.Z Safari/537.36
또는

Mozilla/5.0 (compatible; Yeti/1.1; +https://naver.me/spd)
그 외에 특별한 용도로, 광고주 페이지에서 효과적인 광고 노출을 위한 추가 정보를 수집하는 로봇 (UA : Ads-Naver)과 블로그 에디터 등에서 링크를 삽입할 때 미리보기를 수집하는 로봇 (UA : Blueno) 등의 전문 로봇이 활동하는 경우가 있습니다.
광고용 웹수집 로봇에 대해
요약정보 수집 로봇에 대해

IP 정보로 확인
로봇의 접근은 앞서 설명드린 HTTP header 내의 User-Agent 정보를 이용하여 식별할 수 있습니다. 다만 이 정보만으로 네이버 서버로부터의 접근인지 확신할 수 없다면, 다음 방법으로도 확인이 가능합니다.

역 DNS 조회를 통한 확인
웹서버 로그 혹은 방화벽 접근 기록에 있는 접근자의 IP 주소에 대해 역 DNS 조회를 하여 조회된 도메인이 .naver.com으로 끝나는지 확인합니다.

위에서 얻어진 도메인에 대해서 DNS 조회를 하여 원래 IP 주소와 같은지 확인합니다.

* 예시 (리눅스/MacOS)
$ host 125.209.235.169
169.235.209.125.in-addr.arpa domain name pointer crawl.125-209-235-169.web.naver.com.

$ host crawl.125-209-235-169.web.naver.com
crawl.125-209-235-169.web.naver.com has address 125.209.235.169
* 예시 (Windows, 윈도우키 + R 입력 후 cmd 실행)
C:\Users> nslookup 125.209.235.169
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169

C:\Users> nslookup crawl.125-209-235-169.web.naver.com
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169
목록 대조를 통한 확인
식별하고 싶은 IP 주소를 네이버 검색 로봇이 사용하고 있는 IP 범위 목록(JSON / CIDR 형식)과 대조하여 자동화된 방식으로 식별을 시도할 수 있습니다.
단, 목록 내용은 예고없이 변경될 수 있습니다.

웹마스터 가이드
/
/
웹 사이트를 만들 때
웹 사이트를 만들 때
같은 내용이라면 되도록이면 단일 호스트 명, 단일 URL을 사용하도록 설정해 주세요.
같은 내용을 나타내는 여러 hostname/URL을 만들지 마세요.

여러 hostname 쓴다고 검색에 노출이 될 가능성이 높아지지 않습니다. 똑같은 내용은 중복 판정을 받습니다.
Web cache에 같은 내용으로 중복되는 등 불필요한 인터넷 트래픽을 유발하여 검색로봇이 효과적으로 수집하는 데 방해가 됩니다.
여러 주소를 꼭 써야 한다면 대표(canonical) 주소로 301 redirect 해 주세요.

대표(canonical) 주소란 사람들에게 알리고자 하는 주소이며 호스팅 업체나 플랫폼에 상관없이 바뀌지 않는 고유 주소를 의미합니다.
구 서비스 유지, 호환성 등의 문제로 인하여 같은 내용을 여러 주소로 표현해야 하는 경우에 대표 주소로 redirect 해야 하며, 특히 www가 있는 경우와 없는 경우는 여러분의 선호도에 따라 한 쪽으로 redirect 해야 합니다.
redirect 처리 시 자바스크립트 (예: document.location = 새 URL;)가 아닌 HTTP redirect를 사용해주세요.
대표주소
대표 주소로 301 redirect 가 어렵다면 HTML 마크업 시 rel="canonical" 으로 대표 주소를 기입해주세요.

rel="canonical" 정보는 <link> 태그를 이용하여 작성 가능하며 자세한 내용은 웹 표준 HTML 마크업 가이드를 참고하세요.
대표 주소를 지정한다면 검색 결과에서 여러분의 콘텐츠가 중복 없이 대표 URL로 노출될 수 있는 가능성이 커집니다.
호스트 명 표준을 따라 주세요. 호스트 명에 밑줄글자(underscore) 사용하지 말아주세요.
호스트, 도메인 이름으로 허락된 글자는 알파벳, 숫자 그리고 하이픈(-) 입니다.
hyphen은 맨 앞이나 뒤에 올 수 없습니다. (RFC-952)
한글 도메인이라 불리는 국제화 DNS에서도 hostname으로 underscore를 사용할 수 없습니다.(RFC-5890, 5891, 5892, 5893)
표준이 아닌 도메인 이름은 규정을 느슨히 적용한 웹 브라우저에서는 작동할 수 있으나 다른 곳에서 오동작 할 수 있습니다. 또한, SNS 등에서 링크가 잘못 활성화될 소지가 있습니다.
image
robots.txt를 사용하여 검색로봇이 방문할 수 있도록 허용해주세요
robots.txt 샘플 및 설명

네이버 검색로봇만 접근 가능하게 설정
User-agent: Yeti
Allow: /
모든 검색엔진의 로봇에 대하여 접근 가능하게 설정
User-agent: *
Allow: /
사이트의 루트 페이지만 접근 가능하게 설정
User-agent: *
Disallow: /
Allow: /$
관리자 페이지, 개인 정보 페이지와 같이 검색로봇 방문을 허용하면 안 되는 웹 페이지는 수집 금지로 설정해주세요. 만약, 모든 페이지를 수집 금지로 설정하면 사이트의 어떠한 페이지도 검색 결과에 노출되지 않으므로 수집 대상이 아닌 URL 패턴만 Disallow로 지정하는 것을 권장합니다.
User-agent: Yeti
Disallow: /private*/
네이버 검색로봇에게 /private-image, /private-video 등은 수집하면 안된다고 알려줌

User-agent: *
Disallow: /
모든 검색로봇에게 모든 페이지가 수집 금지임을 알려줌

robots.txt 파일은 반드시 root에 위치해야 합니다.
예) http://www.example.com/robots.txt

robots.txt에서 sitemap.xml의 위치를 알릴 수 있습니다.

User-agent: *
Allow: /
Sitemap: http://www.example.com/sitemap.xml
robots.txt 파일이 없으면 네이버 검색로봇은 사이트 내 모든 문서를 수집 대상으로 간주합니다.
되도록이면 robots.txt 파일을 생성하는 것을 권장하며, 검색엔진에 노출이 안되는 문서의 경우 Disallow 문법을 사용하여 수집 금지 정책을 적용해주세요.
robots.txt에 대한 좀 더 상세한 내용을 알고 싶다면 http://www.robotstxt.org/를 참고해주세요.
전체 사이트의 구조를 알 수 있도록 사이트맵 XML 파일을 만들어 주세요.
사이트맵이란?

사이트맵은 사이트 내의 수집 대상 URL 목록을 담은 XML 형식의 파일입니다.
사이트 내에 수집되어야 할 페이지들을 검색로봇에 알려 줄 수 있습니다.
이미 수집되고 있더라도 추가적인 정보를 검색로봇에 제공하여 더 잘 수집되게 도울 수 있습니다.
sitemap.xml 샘플 및 설명

사이트맵에 대한 상세한 내용은 사이트맵 제출 가이드를 참고해주세요.
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
	<loc>http://www.example.com/footer.html</loc>
	<lastmod>2019-08-26T11:16:53+09:00</lastmod>
	<changefreq>weekly</changefreq>
	<priority>0.8</priority>
  </url>
</urlset>
오류 페이지는 HTTP 규약에 맞게 작성해 주세요
오류 페이지는 의미 있는 HTTP 응답 코드를 유지해 주세요.

404 오류 페이지는 404 Not Found HTTP response 하게 만들어 주세요.
오류 page로 redirect 하도록 하지 말아 주세요.
“페이지가 없습니다”라는 내용만 출력하고 HTTP 200 OK return 하지 말아 주세요.
사람 뿐 아니라 HTTP 규약을 준수하는 S/W가 이해할 수 있게 해 주세요.
image
사이트 맞춤형 오류 페이지(Custom error page)를 통해 사이트 방문자에게 자연스러운 오류 안내를 해주세요.

잘못된 URL입력이나 권한 밖의 페이지에 접근했을 때 사용자에게 안내하는 페이지 입니다.
404 Not Found: 자료 없음. URL 잘못 입력했음. 옛날 URL임.
403 Forbidden : 권한 없음. 막혔음.
503 Service Not Available : 서버가 많이 힘듭니다. 버틸 수가 없습니다.
위 경우에도 내용만 출력하고 HTTP 200 OK return 하지 말아 주세요.
image
모바일 친화적인 사이트를 만들어주세요.
반응형 웹 사이트

반응형 웹은 웹 브라우저가 웹문서의 가로폭을 기기의 스크린 크기에 맞게 자동적으로 조절하는 기법입니다. 반응형 웹은 데스크톱이나 모바일 환경에 상관없이 동일한 URL을 사용하여 기기에 최적화된 콘텐츠를 제공할 수 있습니다. 네이버 검색엔진은 반응형 웹 사이트를 권장하며 자세한 내용은 웹 표준 HTML 마크업 가이드의 모바일 사용성 항목을 참고하세요.
별도의 모바일 URL 제공

만약 여러분의 데스크톱 사이트에 대응하는 별도의 모바일 사이트가 존재한다면, 사용자가 모바일 웹 브라우저로 데스크톱 사이트에 접근시 자동으로 모바일 사이트로 redirect를 처리해야 합니다. 대부분의 웹 브라우저는 자신만의 이름(User Agent Name)을 가지고 있으므로 여러분의 사이트가 모바일 환경을 인식할 수 있도록 사용성을 개선해 주세요.
일반적으로 별도의 모바일 URL을 제공하는 사이트의 경우 데스크톱 사이트와 동일한 콘텐츠를 제공하는 경우가 많습니다. 이 경우 모바일 사이트의 페이지에 1:1로 대응되는 데스크톱 사이트의 URL을 명시적으로 지정하는 것을 권장합니다. 자세한 내용은 웹 표준 HTML 마크업 가이드의 모바일 사용성 항목을 참고하세요.

robots.txt 설정하기
robots.txt는 검색로봇에게 사이트 및 웹페이지를 수집할 수 있도록 허용하거나 제한하는 국제 권고안입니다. IETF에서 2022년 9월에 이에 대한 표준화 문서를 발행하였습니다.

robots.txt 파일은 항상 사이트의 루트 디렉터리에 위치해야 하며 로봇 배제 표준을 따르는 일반 텍스트 파일로 작성해야 합니다. 네이버 검색로봇은 robots.txt에 작성된 규칙을 준수하며, 만약 사이트의 루트 디렉터리에 robots.txt 파일이 없다면 모든 콘텐츠를 수집할 수 있도록 간주합니다.

다만, 별도의 웹 마스터와의 수집 조건에 대한 협약이 있는 경우, 광고주 정보 취득, 링크 미리보기 생성 등 특수한 용도의 로봇 등은 robots.txt 내의 규칙을 참조하지 않거나 완벽하게 준수하지 않을 수 있습니다. 그러므로 개인 정보를 포함하여 외부에 노출되면 안 되는 콘텐츠의 경우 이 설정 외에 로그인 기능 등 다른 방법을 통하여 보호하거나 다른 차단 방법을 사용해야 합니다.

robots.txt 위치
robots.txt 파일은 반드시 사이트의 루트 디렉터리에 위치해야 하며 텍스트 파일 (text/plain) 로 접근이 가능해야 합니다.

예) http://www.example.com/robots.txt
HTTP 응답코드에 따른 처리
사이트의 robots.txt에 네이버 검색로봇이 접근하였을 때 정상적인 2xx 응답 코드를 전달해주세요. 검색로봇은 HTTP 응답 코드에 따라 아래와 같이 동작합니다.

응답 코드 그룹	설명
2xx	Successful	로봇 배제 표준을 준수하는 규칙을 해석하여 사용합니다. 만약, robots.txt가 HTML 문서로 반환된다면 그 안에 유효한 규칙이 있더라도 robots.txt가 없음 (모두 허용)으로 해석될 수도 있습니다. 그러므로, 로봇 배제 규칙을 따르는 일반 텍스트 파일 (text/plain)로 작성하는 것을 권장합니다.
3xx	Redirection	HTTP redirect에 대하여 5회까지 허용하며 그 이상의 redirection 발생 시 중단 후 '모두 허용'으로 해석합니다. HTML 및 JavaScript를 통한 redirection은 해석되지 않으므로 주의가 필요합니다.
4xx	Client Error	'모두 허용'으로 해석합니다.
5xx	Server Error	'모두 허용하지 않음'으로 해석합니다. 다만, 이전에 정상적으로 수집된 robots.txt 규칙이 있다면 일시적으로 사용될 수 있습니다.
robots.txt 규칙 예제
robots.txt 파일에 작성된 규칙은 같은 호스트, 프로토콜 및 포트 번호 하위의 페이지에 대해서만 유효합니다. http://www.example.com/robots.txt의 내용은 http://example.com/ 와 https://example.com/에는 적용되지 않습니다.

대표적인 규칙은 아래와 같으며 사이트의 콘텐츠 성격에 맞게 변경해주세요.

다른 검색엔진의 로봇에 대하여 수집을 허용하지 않고 네이버 검색 서비스용 검색로봇만 수집 허용으로 설정합니다.
User-agent: *
Disallow: /
User-agent: Yeti
Allow: /
모든 검색엔진의 로봇에 대하여 수집 허용으로 설정합니다.
User-agent: *
Allow: /
사이트의 루트 페이지만 수집 허용으로 설정합니다.
User-agent: *
Disallow: /
Allow: /$
관리자 페이지, 개인 정보 페이지와 같이 검색로봇 방문을 허용하면 안 되는 웹 페이지는 수집 비허용으로 설정해주세요. 아래 예제는 네이버 검색 서비스용 검색로봇에게 /private-image, /private-video 등은 수집하면 안 된다고 알려줍니다.
User-agent: Yeti
Disallow: /private*/
모든 검색로봇에게 사이트의 모든 페이지에 대하여 수집을 허용하지 않는다고 알려줍니다. 이 예제는 사이트의 어떠한 페이지도 수집 대상에 포함되지 않으므로 권장하지 않습니다.
User-agent: *
Disallow: /
파비콘(favicon) 수집을 허용해 주세요
검색 로봇은 파비콘 파일을 소스가 되는 웹 페이지 컨텐츠의 일부로 판단합니다. 따라서, 기본적으로 검색 로봇의 수집이 허용된 웹 페이지임에도 불구하고, 포함된 파비콘이 포함된 경로가 robots.txt 설정되는 모순된 경우, 로봇이 예외적으로 규칙을 따르지 않고 수집을 시도하는 경우가 있습니다.

따라서 robots.txt 설정시, 가능하면, 혼란이 없도록 검색에 노출되는 문서의 파비콘은 문서와 동일하게 설정하거나 기본적으로 수집을 허용하도록 해주세요. 검색에 노출되는 파비콘과 관련된 좀 더 자세한 내용은 파비콘 마크업 가이드 을 참고하세요.

자바스크립트 및 CSS 파일 경로도 확인해주세요
로봇은 HTML 웹 문서 만이 아니라, 그 안에 포함된 자바스크립트나 CSS 등 포함된 다른 리소스에도 때때로 설정과 관계 없이 접근을 시도하는 경우가 있습니다. HTML 만으로는 올바른 내용 해석이 어렵거나, 리소스도 접근하여 분석하지 않으면 전혀 웹마스터의 의도와 다른 문서로 해석될 가능성이 있는 문서 등이 늘고 있기에, 현대적인 검색 로봇은 이들을 수집 허용된 웹 문서의 한 부분으로 간주하기 때문입니다.

따라서 이들 리소스 URL도 참조하는 웹 문서와 규칙을 동일하게 설정해주시거나 기본적으로 수집을 허용해주시도록 설정 부탁드립니다. 자세한 내용은 자바스크립트 검색 최적화 문서를 참고하시기 바랍니다.

sitemap.xml 지정
내 사이트에 있는 페이지들의 목록이 담겨있는 sitemap.xml의 위치를 robots.txt에 기록해서 검색 로봇이 내 사이트의 콘텐츠를 더 잘 수집할 수 있도록 도울 수 있습니다.

User-agent: *
Allow: /
Sitemap: http://www.example.com/sitemap.xml
웹마스터도구의 robots.txt 도구를 활용하세요
웹마스터도구에서 제공하는 robots.txt 도구를 활용하면 보다 쉽게 사이트의 robots.txt 파일을 관리할 수 있으며 아래와 같이 2가지 기능을 제공합니다.

1. robots.txt 수집 및 검증
robots.txt 수집 및 검증

사이트의 루트 디렉터리에 있는 robots.txt 파일을 수정한 뒤 검색로봇에게 빠르게 알리고 싶다면 수집 요청을 눌러주세요
설정된 로봇룰에 따라서 웹 페이지의 수집 가능여부를 테스트할 수 있습니다.
2. robots.txt 간단 생성
robots.txt 간단생성

robots.txt 파일을 간단하게 생성 후 다운로드할 수 있습니다. 다운로드한 robots.txt 파일을 사이트의 루트 디렉터리에 업로드 후 위 1번의 수집 요청을 실행하면 네이버 검색로봇이 바로 인식할 수 있습니다.

웹 페이지의 이동
사이트 변경 등의 이유로 웹 페이지를 다른 주소로 redirect (이동)하는 경우에는 주의하실 점이 있습니다.

자바스크립트를 이용한 redirect 처리는 자제해 주세요.
사이트 및 페이지의 redirect 처리 시 자바스크립트 형식을 사용하는 경우 일반적으로 검색로봇이 이해하기 어려운 형식이라 해당 링크가 어떤 URL로 redirect 되는지 잘 파악하기 어려운 문제가 있습니다.

또한 redirect 정보를 숨기는 형태의 페이지는 주로 스팸성 문서에서 많이 발견되고 있어서, 양질의 좋은 페이지임에도 스팸 문서 처리 과정에서 불필요하게 중요도가 낮은 것으로 저평가를 받을 수도 있습니다.

따라서 url 링크 또는 redirect 처리 시에는 자바스크립트 함수를 사용하지 않는 것이 검색로봇에게 올바른 정보를 알려주는 데 도움이 됩니다.

만약 해당 주소가 redirect 되는 것을 방문자에게 알리고 싶다면 meta refresh를 사용할 수도 있으며, 네이버의 검색 로봇은 meta refresh를 인식합니다. 다만, HTML 4.01 표준에서는 meta refresh 를 사용하지 않기를 권장하고 있으므로 되도록이면 HTTP redirect를 사용해 주시기 바랍니다.

잘못된 redirect 사용 예시
var timerld = setTimeout("move()", 500);
function move() {
  location.href = "http://new.site.com/";
  timerld = 0;
}
meta refresh 사용 예시
<meta http-equiv="refresh" content="10;url=http://new.site.com">
사이트나 페이지를 변경할 경우 301 Permanently Moved 형식의 redirect를 사용해 주세요.
사이트 또는 내부 페이지의 URL을 변경할 경우 검색로봇은 기존 사이트 또는 그 내부 페이지와 다른 신규 정보인 것으로 잘못 판단할 수 있습니다. 또한 이후의 중복 정보 처리과정에서 기존의 좋은 검색 정보 및 검색 이용자 선호도 정보가 사용되지 못할 수 있습니다.

이렇게 URL 변경 시에는 기존의 URL을 검색로봇에게 함께 알려줘서 기존 정보를 잘 활용할 수 있도록 하는 것이 검색 노출에 큰 도움이 됩니다.
알려주는 방법은 http 규약에 따라 301 permanently moved 형식의 redirect 를 사용하여 서버에서 응답해주시기 바랍니다.

Old host 전체를 New host 의 top page 로 redirect (apache 웹서버)
Redirect 301 / http://new.site.com
혹은
Redirect permanent http://new.site.com
example.com/* 을 www.example.com/* 으로 redirect (apache 웹서버)
RewriteEngine On
RewriteCond %{HTTP_HOST} example.com
RewriteRule ^([^/]*})$ http://www.example.com [R=301,L]
사이트의 장애나 잠시 페이지를 변경할 경우 302 Temporarily Moved 형식의 redirect를 사용해 주세요.
사이트에 일시적인 장애가 있거나 잠시 페이지를 변경하는 경우 검색로봇은 사이트와 그 내부 페이지에 대해 응답을 받지 못할 경우 해당 페이지가 삭제된 것으로 잘못 판단할 수 있습니다. 일시적인 페이지 변경을 인지하지 못하여 다른 정보로 변경된 것으로 판단하여 기존 정보를 삭제할 수도 있습니다.

이렇게 원하지 않는 기존 검색로봇이 수집한 원래 페이지의 정보를 유지하기를 원하신다면, 사이트에 일시적인 장애가 있거나 잠시 페이지를 변경 시 http 규약에 따라 302형식의 redirect를 사용하여 서버에서 응답해주시기 바랍니다.

공사중인 페이지로 redirect (apache 웹서버)
Redirect / /under_const.html
혹은
Redirect temp / /under_const.html
연속된 리다이렉션 횟수는 최소화해주세요.
페이지가 연속해서 리다이렉션하는 경우로, 리다이렉션 횟수가 검색로봇에 설정된 범위를 초과하는 상태를 말합니다. 연속해서 연결되는 리다이렉션 횟수는 최대 5회 이상을 넘기지 않는 것을 추천합니다. 또한, 프로그램 문제로 자기 자신을 무한해서 반복해서 리다이렉션도 존재하는데요 이 경우에도 검색로봇이 수집하지 않습니다.

가능한 연속된 리다이렉트의 사용은 최소화를 부탁드리며, 리다이렉션 문제는 여러분이 사용하는 브라우저에서 눈으로 확인하기에는 한계점이 존재하므로 브라우저에서 제공하는 개발자 도구를 활용하거나 사이트의 기술담당자와 같이 문제를 해결하는 것을 권장합니다.

이동하는 과정 및 최종 랜딩 URL은 수집 가능한 정상적인 페이지이어야 합니다.
간혹 리다이렉트되는 최종 랜딩 URL이 존재하지 않는 페이지이거나 검색로봇이 접근하지 못하도록 설정한 경우가 있습니다. 사이트 개편 등의 사유로 인하여 정상적인 콘텐츠를 담고 있는 URL 주소가 변경되었다면 변경 전과 변경 후에 URL이 올바르게 리다이렉션으로 연결되어 있는지 확인이 필요합니다.

특히 여러번 연결된 리다이렉션의 경우, 연결 과정에 있는 모든 주소들은 수집이 가능해야 합니다. robots.txt 설정 등에 착오가 있어, 중간에 수집이 불가능한 URL이 섞여있는 경우, 처음과 최종 URL이 수집 가능하다고 하더라도 수집이 안되는 경우가 발생할 수 있으므로 주의해 주십시오.

사이트의 성능 혹은 내부 정책 문제로 잠시 동안 네이버 검색로봇을 차단하고 싶을 때는 웹마스터도구의 트래픽 제한을 사용하세요.
일시적으로 네이버 검색로봇의 접근을 막아야 하는 경우 웹마스터도구의 수집 설정에서 트래픽 제한량을 최소로 지정하는 것을 권장합니다. robots.txt로 검색로봇을 차단하는 방법도 있지만 이 경우 해당 기간 동안에 수집된 사이트 내의 문서는 네이버 검색에서 제외될 수 있습니다.

리다이렉션의 변경 반영은 시간이 걸리는 경우가 있습니다.
사이트 개편 등으로 리다이렉션을 새로 설정하거나 변경하는 경우, 검색 결과 반영에 시간이 걸리는 경우가 있습니다.

검색 로봇은 개별 문서를 수집한 이후에 별도의 과정을 통해서 모아진 정보를 종합해 리다이렉트나 사이트 구조를 분석하고 업데이트하므로, 이 과정에서 정보의 갱신에 시간이 걸릴 수 있습니다. 따라서 사이트 개편 등에 따라 홈페이지 주소 등이 변경되는 경우, 가능하시면 한동안은 기존 검색에 노출되던 URL들에도 리다이렉트를 설정하여, 검색 엔진의 업데이트에 관계 없이 서비스 제공이 가능하도록 하시면 좋습니다.

검색로봇 확인 방법
웹페이지를 방문하는 프로그램이나 앱은 자기 자신만의 이름을 User-Agent에 명시합니다. 통상의 웹 브라우저 소프트웨어뿐만이 아니라 검색로봇도 자신만의 User-Agent 이름을 가지고 있습니다.

많은 웹사이트들은 일부 악의적인 접근을 막기 위하여 방화벽을 운영하고 있습니다. 만약 내 사이트가 검색에 노출되지 않거나 정보가 변경되지 않는다면, 이러한 목적을 위해 활동하는 로봇이 User-Agent 이름이나 IP 주소 등 방화벽의 제한 설정 때문에 접근이 차단되지는 않았는지 점검해야합니다.

네이버가 운용하는 로봇인지를 구분하기 위해서는 User-Agent를 활용하여 구분하는 방법과 역 DNS 조회를 통한 IP로 확인하는 방법이 있습니다.

User-Agent 이름으로 확인
일반적인 네이버 검색 서비스를 위해 웹 문서를 수집하는 검색로봇은 Yeti라고 불리우며,
아래와 같은 User-Agent 이름을 사용합니다. 세부 버전 등은 사전 예고 없이 변경될 수 있습니다.

Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko; compatible; Yeti/1.1; +https://naver.me/spd) Chrome/W.X.Y.Z Safari/537.36
또는

Mozilla/5.0 (compatible; Yeti/1.1; +https://naver.me/spd)
그 외에 특별한 용도로, 광고주 페이지에서 효과적인 광고 노출을 위한 추가 정보를 수집하는 로봇 (UA : Ads-Naver)과 블로그 에디터 등에서 링크를 삽입할 때 미리보기를 수집하는 로봇 (UA : Blueno) 등의 전문 로봇이 활동하는 경우가 있습니다.
광고용 웹수집 로봇에 대해
요약정보 수집 로봇에 대해

IP 정보로 확인
로봇의 접근은 앞서 설명드린 HTTP header 내의 User-Agent 정보를 이용하여 식별할 수 있습니다. 다만 이 정보만으로 네이버 서버로부터의 접근인지 확신할 수 없다면, 다음 방법으로도 확인이 가능합니다.

역 DNS 조회를 통한 확인
웹서버 로그 혹은 방화벽 접근 기록에 있는 접근자의 IP 주소에 대해 역 DNS 조회를 하여 조회된 도메인이 .naver.com으로 끝나는지 확인합니다.

위에서 얻어진 도메인에 대해서 DNS 조회를 하여 원래 IP 주소와 같은지 확인합니다.

* 예시 (리눅스/MacOS)
$ host 125.209.235.169
169.235.209.125.in-addr.arpa domain name pointer crawl.125-209-235-169.web.naver.com.

$ host crawl.125-209-235-169.web.naver.com
crawl.125-209-235-169.web.naver.com has address 125.209.235.169
* 예시 (Windows, 윈도우키 + R 입력 후 cmd 실행)
C:\Users> nslookup 125.209.235.169
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169

C:\Users> nslookup crawl.125-209-235-169.web.naver.com
Server: 168.126.63.1
Address: 168.126.63.1

Name: crawl.125-209-235-169.web.naver.com
Address: 125.209.235.169
목록 대조를 통한 확인
식별하고 싶은 IP 주소를 네이버 검색 로봇이 사용하고 있는 IP 범위 목록(JSON / CIDR 형식)과 대조하여 자동화된 방식으로 식별을 시도할 수 있습니다.
단, 목록 내용은 예고없이 변경될 수 있습니다.

SEO 기본 가이드
웹사이트 사용자에게 도움되는 방향으로 사이트와 사용자 환경 개선을 목표로 최적화해야 합니다.
네이버 검색로봇도 사용자에 해당되며 검색엔진 최적화(SEO)를 통해 네이버 검색로봇이 사이트의 콘텐츠를 더 잘 이해하도록 도울 수 있습니다.
검색엔진 최적화(SEO)가 잘 된 콘텐츠는 네이버 검색로봇의 분석에 용이하며, 검색결과에 더 나은 콘텐츠를 제공하는데 도움이 됩니다.

<title> 요소 찾을수 없음
콘텐츠 주제를 나타낼 정확하고 고유한 제목을 만들어야 합니다.
<title> 요소는 네이버 검색로봇에게 페이지의 주제가 무엇인지 알려줍니다.
HTML 문서 <head> 요소 안에 <title> 요소를 추가해, 페이지 주제를 나타내는 정확하고 고유한 제목 텍스트를 작성합니다.

<meta name="description"> 설명 누락
meta description 태그는 페이지 내용을 요약하여 제공합니다.
페이지의 meta description 태그는 페이지의 요약 내용을 여러 문장으로 구성하여 작성합니다. 검색결과의 스니펫으로도 사용되니 각 페이지마다 요약 내용을 입력해주면 좋습니다.
meta description 태그는 HTML 문서 <head> 요소 안에 있습니다.

<title> 요소가 2개 이상 발견
<title> 요소에는 콘텐츠 주제를 나타낼 정확하고 고유한 제목이 있어야합니다.
HTML 문서에 <title> 요소가 2개 이상 작성된 경우, 네이버 검색로봇은 어떤 <title>이 페이지 콘텐츠 주제에 부합하는지 분석해야합니다.
이 경우 검색로봇이 추가 작업을 해야하며 좋지 못 한 페이지로 분류할 수 있습니다.

<title> 요소에 동일한 제목인 웹문서 다수 발견
<title> 요소에는 콘텐츠 주제를 나타낼 정확하고 고유한 제목이 있어야합니다.
사이트내 여러 페이지들이 동일한 <title>을 가진 경우, 검색로봇은 어떤 페이지가 <title>에 가장 적합한지 분석해야합니다.
이런 <title> 요소들은 콘텐츠 주제에 적합하지 않고, 어떤 페이지를 검색결과로 노출해야할지 명확하지 않습니다.
각 페이지마 고유한 <title> 요소를 작성하는 것을 지향해주세요.

<title> 요소 텍스트 길이 확인필요
<title> 요소는 페이지 콘텐츠 제목을 나타냅니다. 제목을 매우 긴 텍스트로 작성하는 것은 옳지 않습니다.

<meta name="description"> 태그에 동일 설명문 발견
<title> 요소에는 콘텐츠 주제를 나타낼 정확하고 고유한 제목을 작성하듯이, meta description도 고유한 요약 내용으로 작성해야합니다.
여러 페이지에 동일한 meta description이 있다면 네이버 검색 로봇이 유의미하지 않은 내용으로 판단하거나 중복된 문서로 분류하여 노출에 영향을 받을 수 있습니다.

<H1> 요소가 2개 이상 발견
<H1> 요소는 페이지 콘텐츠를 나타내는 소제목으로 사용되기에, HTML 문서에서 2개 이상 발견된다면 네이버 검색로봇이 이해하기 어려운 구조가 됩니다.

Alt 속성 누락
콘텐츠 이미지에 설명(Alt) 속성이 있으면 네이버 검색로봇이 이해하기 좋은 구조입니다.
페이지 콘텐츠와 이미지에 대한 주제를 잘 해석할 수 있습니다.